{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/velizhask/PemrosesanBahasaAlami-nlp/blob/main/Representasi%20Teks2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdfaziyVIqAt"
      },
      "source": [
        "#Representasi Teks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vrx6l6EpTOM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-YDPrlbxTOHG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mszV3_Ul1nj"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mwr_pJq4mtdd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-rKEkxrnlht"
      },
      "outputs": [],
      "source": [
        "# Dataset sederhana\n",
        "data = {\n",
        "    'text': [\n",
        "        \"I love this product\",\n",
        "        \"This is the best thing ever\",\n",
        "        \"I hate this item\",\n",
        "        \"Worst purchase I've ever made\",\n",
        "        \"I'm so happy with my purchase\",\n",
        "        \"This is terrible\"\n",
        "    ],\n",
        "    'label': [1, 1, 0, 0, 1, 0]  # 1: positif, 0: negatif\n",
        "}\n",
        "\n",
        "# Membuat DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFpaDuvHLbek"
      },
      "source": [
        "**`Word embedding`** merupakan representasi yang dipelajari untuk teks di mana kata-kata yang memiliki makna sama memiliki representasi yang serupa. Metode Word embedding mempelajari representasi vektor bernilai riil untuk kosakata berukuran tetap yang telah ditentukan sebelumnya dari korpus teks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgXcoRp_JfAx"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenisasi\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Menghapus tanda baca dan stopwords, serta lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "    return tokens\n",
        "\n",
        "# Menerapkan preprocessing pada dataset\n",
        "df['tokenized'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Membuat dan melatih model Word2Vec\n",
        "model = Word2Vec(sentences=df['tokenized'], vector_size=50, window=3, min_count=1, workers=4)\n",
        "\n",
        "# Fungsi untuk mengubah kalimat menjadi vektor rata-rata\n",
        "def get_sentence_vector(sentence):\n",
        "    vector = np.zeros(50)  # ukuran vektor\n",
        "    count = 0\n",
        "    for word in sentence:\n",
        "        if word in model.wv.key_to_index:  # Mengecek apakah kata ada dalam model\n",
        "            vector += model.wv[word]\n",
        "            count += 1\n",
        "    if count > 0:\n",
        "        vector /= count\n",
        "    return vector\n",
        "\n",
        "# Membuat fitur vektor untuk setiap kalimat\n",
        "X = np.array([get_sentence_vector(sentence) for sentence in df['tokenized']])\n",
        "y = df['label'].values\n",
        "\n",
        "\n",
        "worddata = pd.DataFrame(X)\n",
        "worddata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMaEvNaNODjY"
      },
      "source": [
        "**`Sentence Embedding`** Ini mirip dengan penyematan kata, satu-satunya perbedaan adalah sebagai pengganti kata, kalimat direpresentasikan sebagai vektor numerik dalam ruang dimensi tinggi. Tujuan penyematan kalimat adalah untuk menangkap makna dan hubungan semantik antara kata-kata dalam sebuah kalimat, serta konteks di mana kalimat tersebut digunakan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5BzMpHmniVc"
      },
      "outputs": [],
      "source": [
        "pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMjj117X6_EM"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenisasi\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Menghapus tanda baca dan stopwords, serta lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "\n",
        "# Menerapkan preprocessing pada dataset\n",
        "df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Menginisialisasi model Sentence-BERT\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Mendapatkan embedding kalimat\n",
        "X = model.encode(df['processed_text'].tolist())\n",
        "y = df['label'].values\n",
        "\n",
        "sentence_data = pd.DataFrame(X)\n",
        "sentence_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr95CTAmRL0C"
      },
      "source": [
        "`Doc2Vec` digunakan untuk mendapatkan document embeddings, yaitu representasi vektor untuk dokumen atau teks panjang (bukan hanya kalimat pendek). Ini merupakan perpanjangan dari Word2Vec yang bertujuan untuk merepresentasikan seluruh dokumen atau paragraf sebagai sebuah vektor. TaggedDocument digunakan untuk melabeli setiap dokumen atau teks dalam koleksi, karena Doc2Vec belajar melalui dokumen yang sudah diberi tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7Vr0Zh8SRgs"
      },
      "source": [
        "**`Document Embedding`** mengacu pada proses merepresentasikan seluruh dokumen, seperti paragraf, artikel, atau buku, sebagai vektor tunggal. Ini menangkap tidak hanya makna dan konteks kalimat individu tetapi juga hubungan dan koherensi antara kalimat dalam dokumen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKKq6nZ9OWEg"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "# Preprocessing\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenisasi\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Menghapus tanda baca dan stopwords, serta lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "    return tokens\n",
        "\n",
        "# Menerapkan preprocessing pada dataset\n",
        "df['tokenized'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Membuat tagged documents untuk Doc2Vec\n",
        "tagged_data = [TaggedDocument(words=row['tokenized'], tags=[i]) for i, row in df.iterrows()]\n",
        "\n",
        "# Membuat dan melatih model Doc2Vec\n",
        "model = Doc2Vec(vector_size=50, window=3, min_count=1, workers=4, epochs=40)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Mengubah dokumen menjadi vektor\n",
        "X = np.array([model.infer_vector(doc.words) for doc in tagged_data])\n",
        "y = df['label'].values\n",
        "\n",
        "doc_data = pd.DataFrame(X)\n",
        "doc_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZaESvDbSelOj"
      },
      "outputs": [],
      "source": [
        "print(tagged_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U1nUINeSepK"
      },
      "source": [
        "**SOAL PRAKTIKUM**\n",
        "\n",
        "\n",
        "\n",
        "*   Terapkan N-Gram, Word Embedding, Sentence Embedding dan Document Embedding ke data yang diolah pada praktikum pertemuan 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3fzAza4SPNr"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d ucupsedaya/gojek-app-reviews-bahasa-indonesia #copy API command\n",
        "!unzip gojek-app-reviews-bahasa-indonesia.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dalYlBLiSffE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df= pd.read_csv('/content/GojekAppReviewV4.0.0-V4.9.3_Cleaned.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-gKA-2BSg_j"
      },
      "outputs": [],
      "source": [
        "# Mengambil hanya kolom 'review' dan 'rating'\n",
        "df = df[['content', 'score']]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LObovmAwSjpG"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jst_dSjVSnU4"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkgpNdy9Skte"
      },
      "outputs": [],
      "source": [
        "pd.reset_option('display.max_colwidth')\n",
        "df = df.dropna()\n",
        "df = df.drop_duplicates()\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpORxtwQSrCT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6dRoqL2SsD8"
      },
      "outputs": [],
      "source": [
        "# Fungsi untuk menghapus tanda baca\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Konversi ke huruf kecil\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Hapus tanda baca\n",
        "    text = re.sub(r'\\d+', '', text)  # Hapus angka\n",
        "    text = re.sub(r'(.)\\1{1,}', r'\\1', text)  # Menghapus karakter berulang lebih dari 2 kali\n",
        "    return text\n",
        "\n",
        "df['content']=df['content'].apply(clean_text)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDpwnDdkStgF"
      },
      "outputs": [],
      "source": [
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Terapkan fungsi tokenisasi pada setiap elemen di kolom 'content'\n",
        "df['content'] = df['content'].apply(tokenize_text)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbvZPp_iSvsE"
      },
      "outputs": [],
      "source": [
        "!pip install sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuQxxAGSSw1Z"
      },
      "outputs": [],
      "source": [
        "sw = stopwords.words('indonesian')\n",
        "# Fungsi untuk menghapus stopwords dari token-token kata\n",
        "def remove_stopwords(tokens):\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in sw]\n",
        "    return filtered_tokens\n",
        "\n",
        "# Menghapus stopwords dari kolom 'Tokenized Text'\n",
        "df['content'] = df['content'].apply(remove_stopwords)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGfpxfpwSxng"
      },
      "outputs": [],
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Membuat stemmer dari Sastrawi\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stemming(words):\n",
        "    return [stemmer.stem(word) for word in words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fHmyH-JSyx5"
      },
      "outputs": [],
      "source": [
        "# Menerapkan fungsi stemming pada kolom 'content'\n",
        "# Menerapkan stemming hanya pada 5 baris pertama\n",
        "df.loc[:4, 'content'] = df.loc[:4, 'content'].apply(stemming)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxbva7AIS0pe"
      },
      "outputs": [],
      "source": [
        "df['content'] = df['content'].apply(lambda x: ' '.join(x)) #membuat isi content menjadi kalimat kembali\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBZ_e7DOUZjo"
      },
      "outputs": [],
      "source": [
        "# Menerapkan preprocessing pada dataset\n",
        "df['tokenized'] = df['content'].apply(preprocess_text)\n",
        "\n",
        "# Membuat dan melatih model Word2Vec\n",
        "model = Word2Vec(sentences=df['tokenized'], vector_size=50, window=3, min_count=1, workers=4)\n",
        "\n",
        "# Fungsi untuk mengubah kalimat menjadi vektor rata-rata\n",
        "def get_word_vector(word):\n",
        "    vector = np.zeros(50)  # ukuran vektor\n",
        "    count = 0\n",
        "    for word in word:\n",
        "        if word in model.wv.key_to_index:  # Mengecek apakah kata ada dalam model\n",
        "            vector += model.wv[word]\n",
        "            count += 1\n",
        "    if count > 0:\n",
        "        vector /= count\n",
        "    return vector\n",
        "\n",
        "# Membuat fitur vektor untuk setiap kalimat\n",
        "X = np.array([get_word_vector(word) for word in df['tokenized']])\n",
        "y = df['score'].values\n",
        "\n",
        "\n",
        "worddata = pd.DataFrame(X)\n",
        "worddata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU1h-fpweaiw"
      },
      "source": [
        "`# Sentence Embedding `"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tD9zBeTbZtMD"
      },
      "outputs": [],
      "source": [
        "# # Mengambil stopwords\n",
        "sw = stopwords.words('indonesian')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Fungsi preprocessing teks\n",
        "def preprocess_text(text):\n",
        "    # Tokenisasi\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Menghapus tanda baca dan stopwords, serta melakukan stemming\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in sw and word not in string.punctuation]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Menerapkan preprocessing pada dataset\n",
        "df['processed_text'] = df['content'].apply(preprocess_text)\n",
        "\n",
        "# Menginisialisasi model Sentence-BERT\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Mendapatkan embedding kalimat\n",
        "X = model.encode(df['processed_text'].tolist())\n",
        "y = df['score'].values\n",
        "\n",
        "# Menyimpan embedding ke dalam DataFrame\n",
        "sentence_data = pd.DataFrame(X.numpy())\n",
        "\n",
        "# Menampilkan 50 baris pertama\n",
        "sentence_data.head(50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm8THFTJEYwp"
      },
      "outputs": [],
      "source": [
        "!pip install -U tensorflow tensorflow-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3KqooF1EVxo"
      },
      "outputs": [],
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Mengambil stopwords bahasa Indonesia\n",
        "sw = stopwords.words('indonesian')\n",
        "\n",
        "# Fungsi preprocessing teks\n",
        "def preprocess_text(text):\n",
        "    # Tokenisasi\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Menghapus tanda baca dan stopwords\n",
        "    tokens = [word for word in tokens if word not in sw and word not in string.punctuation]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Menerapkan preprocessing pada dataset\n",
        "df['processed_text'] = df['content'].apply(preprocess_text)\n",
        "\n",
        "# Mengunduh Universal Sentence Encoder dari TensorFlow Hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "# Mendapatkan embedding kalimat\n",
        "X = embed(df['processed_text'].tolist())\n",
        "\n",
        "# Menyimpan embedding ke dalam DataFrame\n",
        "sentence_data = pd.DataFrame(X.numpy())  # Convert tensor to numpy\n",
        "\n",
        "# Menampilkan 50 baris pertama embedding\n",
        "sentence_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKsH2zo6ef-7"
      },
      "source": [
        "`# Document Embedding`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbTimzyfVYF8"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "\n",
        "sw = stopwords.words('indonesian')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenisasi\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Menghapus tanda baca dan stopwords, serta lemmatization\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in sw and word not in string.punctuation]\n",
        "    return tokens\n",
        "\n",
        "# Menerapkan preprocessing pada dataset\n",
        "df['tokenized'] = df['content'].apply(preprocess_text)\n",
        "\n",
        "# Membuat tagged documents untuk Doc2Vec\n",
        "tagged_data = [TaggedDocument(words=row['tokenized'], tags=[i]) for i, row in df.iterrows()]\n",
        "\n",
        "# Membuat dan melatih model Doc2Vec\n",
        "model = Doc2Vec(vector_size=50, window=3, min_count=1, workers=4, epochs=40)\n",
        "model.build_vocab(tagged_data)\n",
        "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
        "\n",
        "# Mengubah dokumen menjadi vektor\n",
        "X = np.array([model.infer_vector(doc.words) for doc in tagged_data])\n",
        "y = df['score'].values\n",
        "\n",
        "doc_data = pd.DataFrame(X)\n",
        "doc_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu7yFyqRu-G"
      },
      "source": [
        "Note:\n",
        "\n",
        "Jika data kalian menggunakan bahasa inggris tidak perlu menggunakan module Sastrawi, ganti dengan PorterStemmer dan WordNetLemmatizer kemudian pada proses embbedding jangan lupa tambahkan:\n",
        "\n",
        "Lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "menyesuaikan dengan baris program yang dibutuhkan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVbUL8PgSdpt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}